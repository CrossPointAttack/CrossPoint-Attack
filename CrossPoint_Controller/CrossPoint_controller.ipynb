{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "d9743357",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code is open source to USENIX Security submission only (now). \n",
    "# After the publication of the corresponding thesis, the code will be free to use, modify, and distribute.\n",
    "# Copyright (c) [Anonymous authors] [2023], which will be public after the publication.\n",
    "# Licensed under the MIT License, Version 2.0.\n",
    "# We kindly hope that this code should only be used for academic perspectives, \n",
    "# Please do not abuse this code to launch any attacks. \n",
    "\n",
    "# This code is the CrossPoint controller.\n",
    "# After you collect the bot-config files, you can run this file to reveal hidden links.\n",
    "# Input: 1. bot-config files (JSON) should in the correct place.\n",
    "#        2. Optionally, you can have csv files that represent the result of congestion.\n",
    "#        3. Your budget.\n",
    "# Output: 1. The control group.\n",
    "#         2. The attack bots (using CC,SD or both.)\n",
    "\n",
    "# CrossPoint attacks workflow: \n",
    "\n",
    "# For reasearhers who want to rebuild our experiments in your own environment:\n",
    "    # 1. Run the bot_config in each bot, generating attack_flow JSON files. \n",
    "    # 2. Send these JSON files to the controller. \n",
    "    # 3. In the controller, run controller_CrossPoint to output the suspisous attack flow set. (This file)\n",
    "    # 4. In the controller, find the control group and the attack flow set.\n",
    "    # 5. Run bash_ping in control group bots and suspicious attack flow bots. \n",
    "    # 6. run conrtoller_CrossPoint to find profitable links.\n",
    "\n",
    "# (Fast example) We also give a fast example with configed bots and congestion files:\n",
    "    # 1. Run the main code in folder example.\n",
    "    # It might take few minutes to run (the main part is analyzing the congestion). \n",
    "\n",
    "# The experiment can be done with simulations and experiments. \n",
    "# The only differences is in the step 2: The method how you transfer JSON bot config files. \n",
    "\n",
    "# Using this you can run the bash_ping command on a bot to get the csv ping files.\n",
    "# You should add your destination address in the @ip_list variable.\n",
    "\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "import logging\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopy.distance\n",
    "import networkx as nx\n",
    "import multiprocessing\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt \n",
    "from itertools import combinations\n",
    "from networkx.exception import NetworkXError\n",
    "from networkx.readwrite.json_graph import node_link\n",
    "from networkx.algorithms.operators.unary import reverse\n",
    "\n",
    "\n",
    "DEBUG = True\n",
    "\n",
    "class atf: # attack_flow_class\n",
    "    def __init__(self,src,dst,weight,delay,route,route_nh,route_ip,route_eq,nh_hd,similarity_v,similarity_p):\n",
    "        self.src = src # Self IP address.\n",
    "        self.dst = dst # dst IP address.\n",
    "        self.weight = weight # Size of attack flows, in default it is 1.\n",
    "        self.delay = delay # propagation delay.\n",
    "        self.route = route # physical route of traceroute.\n",
    "        # This attribute is used in experiment for debug and judge the success. \n",
    "        self.route_nh = route_nh # obfuscated route from nethide.\n",
    "        self.route_ip = route_ip # physical route of traceroute IP, same as self.route\n",
    "        # This attribute is used in experiment for debug and judge the success.\n",
    "        self.route_eq = route_eq # obfuscated route from equalnet.\n",
    "        self.nh_hd = nh_hd # The statistical disparities (SD) value.\n",
    "        self.similarity_v = similarity_v # The statistical disparities value.\n",
    "        self.similarity_p = similarity_p # physical SD, Used for debug and draw figures.\n",
    "    \n",
    "    def __str__(self):\n",
    "        mystr = f\"({self.src},{self.dst}),route={self.route}\"\n",
    "        return mystr\n",
    "    \n",
    "    def get_sd_ip(self):\n",
    "        # Used as keys when sort.\n",
    "        return self.similarity_v\n",
    "    \n",
    "    def get_sd_nh(self):\n",
    "        # Used as keys when sort.\n",
    "        return self.nh_hd\n",
    "    \n",
    "    def __lt__(self,other):\n",
    "        # When sort, this function will be rewrite.\n",
    "        return self.similarity_v < other.similarity_v\n",
    "    \n",
    "    def __eq__(self,other):\n",
    "        if self.src == other.src and self.dst == other.dst:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "    def to_json(self):\n",
    "        return json.dumps(self.__dict__)\n",
    "    \n",
    "    @classmethod\n",
    "    def from_json(cls, json_file):\n",
    "        try:\n",
    "            with open(json_file,'r') as f:\n",
    "                data = json.load(f)\n",
    "                datanew = json.loads(data)\n",
    "               # print(datanew,type(datanew))\n",
    "                f.close()\n",
    "            return cls(**datanew)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"The json file {json_file} not exist.\")\n",
    "        return None\n",
    "        \n",
    "    \n",
    "    def to_json_file(self, path):\n",
    "        myjson = json.dumps(self.__dict__)\n",
    "        myname = path+f'Bot_{self.src}_{self.dst}.json'\n",
    "        with open(myname,'w') as f:\n",
    "            json.dump(myjson,f)\n",
    "            f.close()    \n",
    "\n",
    "class edg: # class edge, for convinence evaluation of different atf_set\n",
    "    def __init__(self,src,dst) -> None:\n",
    "        self.src = src\n",
    "        self.dst = dst\n",
    "        self.atf_set = []\n",
    "        self.fd = 0\n",
    "        self.cp_num = 0\n",
    "        self.cp_set = []\n",
    "        self.real_ip = []\n",
    "    \n",
    "    def update_crosspoint_num(self,link_capacity):\n",
    "        self.cp_num = link_capacity - self.fd\n",
    "\n",
    "    def write_atf_set(self,atf_set):\n",
    "        for atf in atf_set:\n",
    "            self.atf_set.append(atf)\n",
    "        self.fd = len(self.atf_set)\n",
    "        return self.fd\n",
    "\n",
    "    def __lt__(self,other):\n",
    "        return self.fd < other.fd\n",
    "\n",
    "    def __eq__(self,other):\n",
    "        if other == None:\n",
    "            return False\n",
    "        l1 = (self.src == other.src and self.dst == other.dst)\n",
    "        l2 = (self.src == other.dst and self.dst == other.src)\n",
    "        return l1 or l2\n",
    "\n",
    "    def __str__(self):\n",
    "        mystr = f\"({self.src},{self.dst}),flow_density = {self.fd},atf_set = \"\n",
    "        myatf_set = []\n",
    "        for a in self.atf_set:\n",
    "            myatf_set.append((a.src,a.dst))\n",
    "        \n",
    "        return mystr + str(myatf_set)\n",
    "    \n",
    "\n",
    "def raw_ping_to_csv(rawping_file,csv_file_name):\n",
    "    # This function will read a rowping_file and output it to csv ping files.\n",
    "    # Read the file of raw ping data. \n",
    "    # If you do not use our ping tool bash_ping, you will get a raw ping data\n",
    "    # Then you can use this function to transfer the raw ping data to csv files.\n",
    "    # '/home/hxb/CAIDA-dataset/myping_/ping_log_sh_5.18'\n",
    "    with open(rawping_file, 'r') as f:\n",
    "        data = f.readlines()\n",
    "    # Extract the desired features\n",
    "    timestamps = []\n",
    "    icmp_seqs = []\n",
    "    times = []\n",
    "    for row in data:\n",
    "        if 'bytes from' in row:\n",
    "            timestamp = re.search(r\"\\[(.*?)\\]\", row).group(1)\n",
    "            icmp_seq = re.search(r\"icmp_seq=(\\d+)\", row).group(1)\n",
    "            time = re.search(r\"time=(.*?) ms\", row).group(1)\n",
    "            timestamps.append(timestamp)\n",
    "            icmp_seqs.append(icmp_seq)\n",
    "            times.append(time)\n",
    "    # Create a DataFrame\n",
    "    df = pd.DataFrame({'time': timestamps, 'seq': icmp_seqs, 'delay': times})\n",
    "    # Save to a new CSV file\n",
    "    df.to_csv(csv_file_name+'.csv', index=False)\n",
    "    return \n",
    "\n",
    "class congestion:\n",
    "    def __init__(self,start_time,congestion_sample,length,coef_start_time,minrtt) -> None:\n",
    "        self.length = length\n",
    "        # Most importantly, the start_time must correct. \n",
    "        # Most importantly, the start_time must correct. \n",
    "        # It is the one of the main feature that influence the accuracy of CrossPoint attacks. \n",
    "        # (One of the authors wrote a bug here ... We spent almost one week to figure it out....)\n",
    "        self.start_time = start_time\n",
    "        self.congestion_sample = congestion_sample\n",
    "        self.coef_sample = [minrtt]*10 + congestion_sample + [minrtt]*10\n",
    "        self.coef_start_time = coef_start_time\n",
    "        \n",
    "    def update_bucket(self,value):\n",
    "        self.congestion_sample = [value if x == -1 else x for x in self.congestion_sample]\n",
    "            \n",
    "    def update_length(self):\n",
    "        # Not use in the latest version for CCS submission\n",
    "        assert(len(self.delta_rtt) == len(self.drop))\n",
    "        self.length = len(self.delta_rtt)\n",
    "        mow = int(self.length / 10 + 1) \n",
    "        \n",
    "    def get_coef_trace(self):\n",
    "        # Not use in the latest version for CCS submission\n",
    "        return self.coef_rtt_before + self.delta_rtt + self.coef_rtt_after \n",
    "\n",
    "    def update_coef_list(self,tr_list):\n",
    "        # Not use in the latest version for CCS submission\n",
    "        assert(self.length!=0)\n",
    "        coef_before_t = max(self.start_time - self.length * 0.1, 0)\n",
    "        coef_after_t = min(self.start_time + self.length * 0.2, len(tr_list.tr_list))\n",
    "        idx_before = tr_list.get_trace_from_time(coef_before_t)\n",
    "        idx_after = tr_list.get_trace_from_time(coef_after_t)\n",
    "        c = 0\n",
    "        while c < self.length:\n",
    "            self.coef_rtt_before.append(tr_list.tr_list[idx_before].rtt)\n",
    "            self.coef_rtt_after.append(tr_list.tr_list[idx_after].rtt)\n",
    "            c +=1\n",
    "            idx_before +=1\n",
    "            idx_after +=1\n",
    "        return 0\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        s = f'Congestion {self.start_time} inteval {self.length} rtt {self.congestion_sample} '\n",
    "        return s\n",
    "\n",
    "class correlated_congestion:\n",
    "    \n",
    "    def __init__(self,start_time,member_congestion):\n",
    "        self.start_time = start_time\n",
    "        self.member_congestion = member_congestion\n",
    "\n",
    "    @classmethod\n",
    "    def from_json(cls, json_str):\n",
    "        json_dict = json.loads(json_str)\n",
    "        return cls(**json_dict)\n",
    "\n",
    "    def to_json(self):\n",
    "        return json.dumps(self.__dict__)\n",
    "\n",
    "\n",
    "def get_minRTT(trace):\n",
    "    # The minimal rtt of a trace represents the propagation delay.\n",
    "    delays = trace[\"delay\"]\n",
    "    return delays.min()\n",
    "\n",
    "def get_maxRTT(trace):\n",
    "    # The maximal rtt of a trace represents the egde value of RTT near a packet loss.\n",
    "    # Therefore, we use the maximal value of rtt to change the lost dropped packet's rtt.\n",
    "    delays = trace[\"delay\"]\n",
    "    return delays.max()\n",
    "\n",
    "def get_local_max(trace,idx):\n",
    "    # Local max delay parameter indicates the edge value of a packet loss. \n",
    "    delays = trace[\"delay\"].iloc[idx-5:idx+5]\n",
    "    return delays.max()\n",
    "\n",
    "def get_loss(trace):\n",
    "    seq = trace[\"seq\"]\n",
    "    c = 0\n",
    "    for idx,s in enumerate(seq):\n",
    "        if idx == 0:\n",
    "            continue\n",
    "        loss_idt = seq[idx] - seq[idx-1]\n",
    "        #print(loss_idt)\n",
    "        if loss_idt > 1 :\n",
    "            c += 1\n",
    "    return c\n",
    "            \n",
    "def get_abnRTT(trace):\n",
    "    d = trace[\"delay\"]\n",
    "    minrtt = get_minRTT(trace)\n",
    "    c = 0\n",
    "    for idx,s in enumerate(d):\n",
    "        if idx == 0:\n",
    "            continue\n",
    "        if s > minrtt + 30:\n",
    "            c += 1\n",
    "    return c    \n",
    "\n",
    "def delay_update(trace,minrtt):\n",
    "    trace.loc[abs(trace[\"delay\"] - minrtt)< 10,'delay'] = minrtt\n",
    "    trace.loc[trace[\"delay\"] < minrtt,'delay'] = minrtt\n",
    "\n",
    "def time_synchronize(trace,propagation_delay_to_target):\n",
    "    # This is a synchonization step for updating the time stamp in the trace.\n",
    "    # This step is optional, it need you know the propagation delay to the target.\n",
    "    # So you need to send extra ping messages and get the minimal delay.\n",
    "    # We reconmand you do that because in evaluations it shows a ~20% performance increases.\n",
    "    # Note that the propagation delay and the min rtt are ms. therefore we * 1000\n",
    "    def algin(row):\n",
    "        return row - minrtt/2 * 0.001 - propagation_delay_to_target * 0.001\n",
    "    \n",
    "    trace[\"time\"] = trace[\"time\"].apply(algin)\n",
    "\n",
    "def init_trace(trace):\n",
    "    def create_time_stamp(start,length):\n",
    "        return [start+0.1 + i*0.1 for i in range(length)]\n",
    "\n",
    "    rtt = trace[\"delay\"]\n",
    "    minrtt = get_minRTT(trace)\n",
    "    idx_loss = trace.index[trace[\"seq\"].diff() > 1]\n",
    "    #print(idx_loss)\n",
    "    while not idx_loss.empty:\n",
    "        idx = idx_loss[0]\n",
    "        #print(idx,seq[idx],seq[idx-1])\n",
    "        diff = trace['seq'][idx] - trace['seq'][idx -1] - 1\n",
    "        insert_df = pd.DataFrame(\n",
    "                                 {\"seq\":range(trace.loc[idx-1,'seq'] + 1, trace.loc[idx,'seq']), \n",
    "                                  'delay':[-1.0] * diff, \n",
    "                                 \"time\":create_time_stamp(trace.loc[idx-1,'time'],diff),\n",
    "                                 \"drop\":1})\n",
    "        trace = pd.concat([trace.iloc[:idx],insert_df,trace.iloc[idx:]]).reset_index(drop=True)\n",
    "        #print(trace.iloc[270:280])\n",
    "        idx_loss = trace.index[trace[\"seq\"].diff() > 1]\n",
    "        #print(idx_loss)\n",
    "    indices = trace.loc[trace['delay'] == -1 ].index\n",
    "    for i in indices:\n",
    "        local_max = get_local_max(trace,i)\n",
    "        trace.loc[i,'delay'] = local_max\n",
    "    #print(trace.iloc[270:280])\n",
    "    return trace\n",
    "\n",
    "def init_congestion(trace_file_name):\n",
    "    # Read the ping csv data and output the congestion() samples\n",
    "    congestion_list = []\n",
    "    trace_raw = pd.read_csv(trace_file_name)\n",
    "    trace = init_trace(trace_raw)\n",
    "    #print(trace.iloc[270:300])\n",
    "    seq = trace[\"seq\"]\n",
    "    rtt = trace[\"delay\"]\n",
    "    timestamp = trace[\"time\"]\n",
    "    minrtt = get_minRTT(trace)\n",
    "    #maxrtt = get_maxRTT(trace)\n",
    "    \n",
    "    #minrtt = 56 # debug\n",
    "    print(f\"minrtt is {minrtt}\")\n",
    "    delay_update(trace,minrtt)\n",
    "    #print(minrtt)\n",
    "\n",
    "    # We set the congestion as delay increase more than 30\\% of the minimal RTT.\n",
    "    # In most congestion control thesis, congestion is any condition when delay > minimal RTT.\n",
    "    delay_idt_thre = min([0.3 * minrtt,30])\n",
    "    #print(delay_idt_thre)\n",
    "    idx = 1 \n",
    "    while idx < len(seq):\n",
    "        # loss indicator to judge whether congestion happens.\n",
    "        loss_idt = seq[idx] - seq[idx - 1]\n",
    "        # delay indicator to judge whether congestion happens.\n",
    "        delay_idt = rtt[idx] - minrtt\n",
    "        # If lost package or delay is high\n",
    "        if loss_idt > 1 or delay_idt > delay_idt_thre:\n",
    "            c_flag = True\n",
    "            congestion_start_time = timestamp[idx]\n",
    "            congestion_sample = []\n",
    "            if loss_idt > 1:\n",
    "                #maxrtt = get_local_max(trace,idx)\n",
    "                maxrtt = max(rtt[idx],rtt[idx-1])\n",
    "                #maxrtt = rtt[idx]\n",
    "                sample = [maxrtt] * (loss_idt - 1)\n",
    "                congestion_sample += sample\n",
    "            if delay_idt > delay_idt_thre:\n",
    "                #congestion_sample.append(delay_idt // delay_idt_thre)\n",
    "                congestion_sample.append(rtt[idx])\n",
    "            leng_idx = idx\n",
    "            # A congestion starts, judge whether the following trace belongs to this congestion.\n",
    "            while c_flag:\n",
    "                idx += 1\n",
    "                if idx >= len(seq):\n",
    "                    break\n",
    "                loss_idt = seq[idx] - seq[idx - 1]\n",
    "                delay_idt = rtt[idx] - minrtt\n",
    "                if loss_idt > 1 or delay_idt > delay_idt_thre:\n",
    "                    c_flag = True\n",
    "                    if loss_idt > 1:\n",
    "                        maxrtt = max(rtt[idx],rtt[idx-1])\n",
    "                        sample = [maxrtt] * (loss_idt - 1)\n",
    "                        congestion_sample += sample\n",
    "                    if delay_idt > delay_idt_thre:\n",
    "                        #congestion_sample.append(delay_idt // delay_idt_thre)\n",
    "                        congestion_sample.append(rtt[idx])\n",
    "                    continue\n",
    "                else:\n",
    "                    c_flag = False\n",
    "                    congestion_length = len(congestion_sample)\n",
    "                    coef_start_time = congestion_start_time - 1.0\n",
    "                    cgt = congestion(congestion_start_time, \\\n",
    "                                     congestion_sample,congestion_length, \\\n",
    "                                    coef_start_time, minrtt)\n",
    "                    congestion_list.append(cgt)\n",
    "        idx += 1\n",
    "    return congestion_list\n",
    "\n",
    "def search_time_trace(trace,start_time):\n",
    "    # Given a start_time (from another bot), we need to search the rtt samples in this bot.\n",
    "    # And then we can use them to judge the coefficiency. \n",
    "    new_df = trace[abs(trace[\"time\"] - start_time) < 0.1]\n",
    "    if not new_df.empty:\n",
    "        idx = (new_df[\"time\"] - start_time).abs().idxmin()\n",
    "        #result = new_df.loc[idx]\n",
    "        #print(\"idx\",idx,\"time {:.3f}\".format(new_df[\"time\"][idx]))\n",
    "        return idx\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def prepare_coef_sample(trace,trace_start_time,length):\n",
    "    # Trace : the whole csv trace\n",
    "    # teace_start_time: the congestion.coef_start_time \n",
    "    # length: the congestion.length\n",
    "    idx_base = search_time_trace(trace,trace_start_time)\n",
    "    if idx_base == None:\n",
    "        return []\n",
    "    if idx_base - 10 < 0 or idx_base + length + 10 > len(trace):\n",
    "        return []\n",
    "    seq = trace[\"seq\"]\n",
    "    rtt = trace[\"delay\"]\n",
    "    timestamp = trace[\"time\"]\n",
    "    drop = trace[\"drop\"]\n",
    "    minrtt = get_minRTT(trace)\n",
    "    delay_update(trace,minrtt)\n",
    "    #idx = trace.loc[trace['time'] == coef_sample_start].index[0]\n",
    "    delay_idt_thre = min([0.3 * minrtt,30])\n",
    "    coef_sample = []\n",
    "    idx = idx_base\n",
    "    drop_count = 0\n",
    "    while len(coef_sample) < length:\n",
    "        coef_sample.append(rtt[idx])\n",
    "        idx += 1\n",
    "        drop_count += drop[idx]\n",
    "    if drop_count > len(coef_sample)*0.8:\n",
    "        return []\n",
    "    nan_flag = True\n",
    "    for t in coef_sample:\n",
    "        if t != minrtt:\n",
    "            nan_flag = False\n",
    "    if nan_flag:\n",
    "        coef_sample[0] -= random.random() / 10\n",
    "    return coef_sample\n",
    "\n",
    "def c_group_congestions(C_group_congestion_file,target_name) -> list:\n",
    "    # Given members in C-group, this function generates a congestion samples list,\n",
    "    # And save it to a JSON file. \n",
    "    # In CrossPoint attacks, this file should be sent to unknown bots for judging.\n",
    "    # This function might be memory starving, depending on how many members in the c-group.\n",
    "    basic_congestion_list = init_congestion(C_group_congestion_file[0])\n",
    "    idx = 1\n",
    "    tr_list = []\n",
    "    c_group_congestion_list = []\n",
    "    # Init trace_list_files\n",
    "    while idx < len(C_group_congestion_file):\n",
    "        print(\"Start inititial traces, \",idx)\n",
    "        tr_raw = pd.read_csv(C_group_congestion_file[idx])\n",
    "        # The init_trace may need a long time to run.\n",
    "        tr = init_trace(tr_raw)\n",
    "        tr_list.append(tr)\n",
    "        idx += 1\n",
    "    print(\"Init congestion and trace finished\")\n",
    "    for c in basic_congestion_list:\n",
    "        bad_congestion_flag = False  \n",
    "        if not c.length > 1:\n",
    "            # Too short congestion, drop\n",
    "            continue\n",
    "        if c.coef_sample == []:\n",
    "            # Bad congestion, might be the \"host unreadable\" in data.\n",
    "            continue\n",
    "        samples = [c.coef_sample]\n",
    "        for tr in tr_list:\n",
    "            bad_congestion_flag = False\n",
    "            c_sample_tmp = prepare_coef_sample(tr, c.start_time -1.0, c.length + 20)\n",
    "            if c_sample_tmp == []:\n",
    "                bad_congestion_flag = True\n",
    "                # If A observes congestion at 9:00 p.m.\n",
    "                # But B's csv file lasts from 8:00 p.m. to 8:30 p.m. (end before the congestion)\n",
    "                # The prepare_coef_sample will return [] to tell the abnormal condition. \n",
    "                break    \n",
    "            samples.append(c_sample_tmp)\n",
    "        if bad_congestion_flag:\n",
    "            continue\n",
    "        co = np.corrcoef(samples)\n",
    "        dis = 1 - co\n",
    "        max_distance = np.max(dis[np.where(~np.eye(dis.shape[0], dtype=bool))])\n",
    "        if max_distance < 0.5 and max_distance >= 0:\n",
    "            cc = correlated_congestion(c.start_time,samples)\n",
    "            c_group_congestion_list.append(cc)\n",
    "        else:\n",
    "            continue\n",
    "    file_name = '_'.join([target_name,'cc'])        \n",
    "    with open(file_name+'.json','w') as f:\n",
    "        for idx,cc in enumerate(c_group_congestion_list):\n",
    "            file_content = cc.to_json()\n",
    "            json.dump(file_content,f)\n",
    "            f.write('\\n')\n",
    "        f.close()\n",
    "            \n",
    "    return c_group_congestion_list\n",
    "\n",
    "def get_cgroup_congestion(filename):\n",
    "    cc_congestion = []\n",
    "    with open(filename) as infile:\n",
    "        for line in infile:\n",
    "            item = json.loads(line)\n",
    "            cc_instance = correlated_congestion.from_json(item)\n",
    "            cc_congestion.append(cc_instance)\n",
    "        infile.close()\n",
    "    return cc_congestion\n",
    "\n",
    "def bot_judge_congestion(bot_congestion_file,cc_congestion_sample):\n",
    "    trace_raw = pd.read_csv(bot_congestion_file)\n",
    "    trace = init_trace(trace_raw)\n",
    "    Flag_of_true_result = 0\n",
    "    print(\"Trace initialize finished. \")\n",
    "    for cc in cc_congestion_sample:\n",
    "        bad_congestion_flag = False\n",
    "        start_time = cc.start_time\n",
    "        length = len(cc.member_congestion[0])\n",
    "        bot_sample = prepare_coef_sample(trace, start_time -1.0, length)\n",
    "        if bot_sample == []:\n",
    "            bad_congestion_flag = True\n",
    "            continue\n",
    "        samples_all = []\n",
    "        for cm in cc.member_congestion:\n",
    "            samples_all.append(cm)\n",
    "        samples_all.append(bot_sample)\n",
    "        co = np.corrcoef(samples_all)\n",
    "        dis = 1 - co\n",
    "        max_distance = np.max(dis[np.where(~np.eye(dis.shape[0], dtype=bool))])\n",
    "        if max_distance < 0.5 and max_distance >= 0:\n",
    "                Flag_of_true_result += 1\n",
    "    print(f\"Find {Flag_of_true_result} evidence of link sharing.\")\n",
    "    return Flag_of_true_result\n",
    "\n",
    "# Trace is a csv file gained from bash_ping shell codes. \n",
    "def get_minRTT(trace):\n",
    "    delays = trace[\"delay\"]\n",
    "    #print(delays.min())\n",
    "    return delays.min()\n",
    "\n",
    "def get_loss(trace):\n",
    "    seq = trace[\"seq\"]\n",
    "    c = 0\n",
    "    for idx,s in enumerate(seq):\n",
    "        if idx == 0:\n",
    "            continue\n",
    "        loss_idt = seq[idx] - seq[idx-1]\n",
    "        #print(loss_idt)\n",
    "        if loss_idt > 1 :\n",
    "            c += 1\n",
    "    return c\n",
    "            \n",
    "def get_abnRTT(trace):\n",
    "    d = trace[\"delay\"]\n",
    "    minrtt = get_minRTT(trace)\n",
    "    c = 0\n",
    "    for idx,s in enumerate(d):\n",
    "        if idx == 0:\n",
    "            continue\n",
    "        if s > minrtt + 30:\n",
    "            c += 1\n",
    "    return c    \n",
    "\n",
    "def init_congestion(trace):\n",
    "    seq = trace[\"seq\"]\n",
    "    rtt = trace[\"delay\"]\n",
    "    minrtt = get_minRTT(t)\n",
    "\n",
    "def bot_init_from_json_files(folder_path):\n",
    "    # This function reads all bots JSON files and config them in an atf_set.\n",
    "    # This file will deterime the attack_set. \n",
    "    atf_set = []\n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        #print(root,dirs,files)\n",
    "        for file in files:\n",
    "            if file.endswith('.json'):\n",
    "                json_file = os.path.join(root, file)\n",
    "               #print(json_file)\n",
    "                atf_instance = atf.from_json(json_file)\n",
    "                #print(atf_instance)\n",
    "                atf_set.append(atf_instance)\n",
    "    return atf_set\n",
    "\n",
    "def bot_init_debugs(phy_topo):\n",
    "    # This function is used for debug and used as a fast run example.\n",
    "    # It generates whole possible bots around a topology. \n",
    "    # Using this you donot need config bot's JSON files; therefore easy for running.\n",
    "    # INPUT: a networkx Graph phy_topo.\n",
    "    # OUTPUT: atttack flow set generates around the topology. \n",
    "    # In detail, the fast flow set uses the shortest path alg to generate routes. \n",
    "    def _attack_flows(phy_topo): \n",
    "            #TODO\n",
    "            #This generation inputs all possible src-dst pairs into defenders.\n",
    "            atk_node_list = []\n",
    "            for node in phy_topo.nodes:\n",
    "                if phy_topo.nodes[node][\"Internal\"] == 1:\n",
    "                    atk_node_list.append(node)\n",
    "            p_tmp = nx.Graph()\n",
    "            p_tmp.add_nodes_from(atk_node_list)\n",
    "            P_CompleteGraph = nx.complete_graph(p_tmp.nodes,create_using = nx.DiGraph)\n",
    "            FlowSet = [edge for edge in P_CompleteGraph.edges] \n",
    "            #print(FlowSet)\n",
    "            return FlowSet  \n",
    "        \n",
    "    def atf_set_init(phy_topo):\n",
    "        _atf_set = _attack_flows(phy_topo)\n",
    "        #print(_atf_set)\n",
    "        atf_set = []\n",
    "        for att_flow in _atf_set:\n",
    "            #print(att_flow)\n",
    "            tmp = atf(src=att_flow[0],dst=att_flow[1])\n",
    "            atf_set.append(tmp)\n",
    "        for atf_flow in atf_set:\n",
    "         # In some topology, where an atf have multiple same-weight routes, \n",
    "         # the shortest_path function randomly choose one.\n",
    "         # So, the shortest_path should be use only once in each experiment.\n",
    "            atf_flow.route = nx.shortest_path(phy_topo,source=atf_flow.src,target=atf_flow.dst)\n",
    "            ip_path = [phy_topo.nodes[node][\"ip\"] for node in atf_flow.route]\n",
    "            atf_flow.route = ip_path\n",
    "            #atf_flow.delay = get_path_delay(atf_flow.route,topo)\n",
    "            atf_flow.weight = 0\n",
    "        return atf_set\n",
    "    \n",
    "    atf_set = atf_set_init(phy_topo)\n",
    "    return atf_set\n",
    "\n",
    "def get_random_bs_pair(number,atf_set):\n",
    "    # Get random number of bot,server pairs from the atf_set.\n",
    "    # This function correspond to the Coremelt+Random attack.\n",
    "    # The number of bot,server pairs is the budget. \n",
    "    random_atf = []\n",
    "    if number > len(atf_set):\n",
    "        return atf_set\n",
    "    random.seed()\n",
    "    random_atf_index = random.sample(range(0,len(atf_set)),number)\n",
    "    for i in random_atf_index:\n",
    "        random_atf.append(atf_set[i])\n",
    "    return random_atf\n",
    "\n",
    "def get_edge_from_list(l):\n",
    "    list_of_edge = []\n",
    "    for idx in range(len(l)-1):\n",
    "        e_tmp = edg(l[idx],l[idx+1])\n",
    "        list_of_edge.append(e_tmp)\n",
    "    return list_of_edge\n",
    "\n",
    "def link_map_actual_route(atf_set):\n",
    "    # Used for debug.\n",
    "    edg_list = []\n",
    "    for atf in atf_set:\n",
    "        edges = get_edge_from_list(atf.route_ip)\n",
    "        for e in edges:\n",
    "            if e not in edg_list:\n",
    "                edg_list.append(e)\n",
    "                inc_fd_in_edg_list(e,atf,edg_list)\n",
    "            else:\n",
    "                inc_fd_in_edg_list(e,atf,edg_list)\n",
    "    return edg_list    \n",
    "\n",
    "\n",
    "def link_map(atf_set,route_name):\n",
    "    def _pre(ip):\n",
    "        ip_p = ip.split('.')[:-1]\n",
    "        ip_prefix = '.'.join(ip_p)\n",
    "        return ip_prefix\n",
    "    \n",
    "    def _get_edge_from_list(l):\n",
    "        list_of_edge = []\n",
    "        for idx in range(len(l)-1):\n",
    "            if route_name =='nh':\n",
    "                e_tmp = edg(l[idx],l[idx+1])\n",
    "            else:\n",
    "                e_tmp = edg(_pre(l[idx]),_pre(l[idx+1]))\n",
    "            \n",
    "            list_of_edge.append(e_tmp)\n",
    "        return list_of_edge\n",
    "    \n",
    "    edg_list = []\n",
    "    for atf in atf_set:\n",
    "        if route_name == 'eq':\n",
    "            edges = _get_edge_from_list(atf.route_eq)\n",
    "        elif route_name == 'nh':\n",
    "            edges = _get_edge_from_list(atf.route_nh)\n",
    "        else:\n",
    "            edges = _get_edge_from_list(atf.route)\n",
    "        for e in edges:\n",
    "            if e not in edg_list:\n",
    "                edg_list.append(e)\n",
    "                inc_fd_in_edg_list(e,atf,edg_list)\n",
    "            else:\n",
    "                inc_fd_in_edg_list(e,atf,edg_list)\n",
    "    return edg_list\n",
    "\n",
    "def get_individual_atfs(edg,atf_set):\n",
    "    return [atf for atf in atf_set if atf not in edg.atf_set]\n",
    "\n",
    "def get_number_of_bots(candi_atfs,number):\n",
    "    if number <= 0:\n",
    "        return []\n",
    "    else:\n",
    "        return candi_atfs[0:number]\n",
    "\n",
    "def link_map_crossfire(atf_set):\n",
    "    edg_list = []\n",
    "    for atf in atf_set:\n",
    "        edges = get_edge_from_list(atf.route_eq)\n",
    "        for e in edges:\n",
    "            if e not in edg_list:\n",
    "                edg_list.append(e)\n",
    "                inc_fd_in_edg_list(e,atf,edg_list)\n",
    "            else:\n",
    "                inc_fd_in_edg_list(e,atf,edg_list)\n",
    "    return edg_list\n",
    "\n",
    "# def sort_with_ohd(indv):\n",
    "#     indv.sort(reverse=True)\n",
    "#     return \n",
    "\n",
    "def _pre(ip):\n",
    "    ip_p = ip.split('.')[:-1]\n",
    "    ip_prefix = '.'.join(ip_p)\n",
    "    return ip_prefix\n",
    "\n",
    "def get_subnet_edge_from_list(l):\n",
    "    def _pre(ip):\n",
    "        ip_p = ip.split('.')[:-1]\n",
    "        ip_prefix = '.'.join(ip_p)\n",
    "        return ip_prefix\n",
    "    list_of_edge = []\n",
    "    for idx in range(len(l)-1):\n",
    "        e_tmp = edg(_pre(l[idx]),_pre(l[idx+1]))\n",
    "        list_of_edge.append(e_tmp)\n",
    "    return list_of_edge\n",
    "\n",
    "def botnum_success(atf_set,e_tar,route_name):\n",
    "    c = 0\n",
    "    for atf in atf_set:\n",
    "        if route_name =='eq':\n",
    "            e_list = get_edge_from_list(atf.route_ip)\n",
    "        else:\n",
    "            e_list = get_edge_from_list(atf.route)\n",
    "        if e_tar in e_list:\n",
    "            c += 1\n",
    "    return c\n",
    "\n",
    "\n",
    "def inc_fd_in_edg_list(edg,atf,edg_list):\n",
    "    for e in edg_list:\n",
    "        if e == edg:\n",
    "            e.atf_set.append(atf)\n",
    "            e.fd +=1\n",
    "    return edg_list\n",
    "\n",
    "def gen_control_group(atf_set,target_edg):\n",
    "    candidates = []\n",
    "    for atf in atf_set:\n",
    "        edges = get_edge_from_list(atf.route)\n",
    "        if target_edg in edges:\n",
    "            candidates.append(atf)\n",
    "    i = 0\n",
    "    current_set_length = 10000\n",
    "    current_control_group = []\n",
    "    while i < len(candidates):\n",
    "        combs = list(combination(candidates,i))\n",
    "        for comb in combs:\n",
    "            e = []\n",
    "            for atf in comb:\n",
    "                e.append(get_edge_from_list(atf.route))\n",
    "            shared_links = set(e)\n",
    "            if len(shared_links) < current_set_length:\n",
    "                current_set_length = len(shared_links)\n",
    "                current_control_group = [a for a in comb]\n",
    "                if len(shared_links) == 1:\n",
    "                    break\n",
    "        i += 1 \n",
    "    return current_control_group\n",
    "\n",
    "def assign_ip_addr_randomly(phy_topo,addr):\n",
    "    # used for debug\n",
    "    #addr = './CAIDA dataset/as_id_node/ipaddr_13576.csv'\n",
    "    df = pd.read_csv(addr)\n",
    "    random_node = random.sample(range(0,len(df)),len(phy_topo.nodes))\n",
    "    for x_id,node in zip(random_node,phy_topo.nodes):\n",
    "        ip_list = df.loc[x_id,'ip_addr']\n",
    "        ip_list = ip_list[1:-1].split(', ')\n",
    "        ip_list = list(set(ip_list))\n",
    "        ip_list.remove('nan')\n",
    "        ip_list_formal = [ip[1:-1] for ip in ip_list]\n",
    "        xx_id = random.randint(0,len(ip_list)-1)\n",
    "        phy_topo.nodes[node][\"ip\"] = ip_list_formal[xx_id]\n",
    "        if DEBUG:\n",
    "            phy_topo.nodes[node][\"ip_list_debug\"] = ip_list_formal\n",
    "    return phy_topo\n",
    "\n",
    "def CrossPoint_Attack_SD(folder_path,budget,route_name):\n",
    "    # Init attack flow set. \n",
    "    print(\"Start attack..\")\n",
    "    atf_set = bot_init_from_json_files(folder_path)\n",
    "    print(\"Init atf set finished.\")\n",
    "    # A simple value of example in Abilene:\n",
    "    Link_capacity = 15\n",
    "    # If you want to run a fast workflow instead of using real probed bot JSON files:\n",
    "    # You can try the debug mode\n",
    "    if DEBUG:\n",
    "        phy_topo = nx.read_gml(\"Abilene.gml\",label='id')\n",
    "        # link capacity is a value that influence the succ rate of the attack.\n",
    "        # As we addressed in Figure.8 and 9 in our thesis.\n",
    "        # for a simple exmple you can set:\n",
    "        \n",
    "        # The following is want we use in experiments:\n",
    "        # A typical value is 80% of the links are secure (according to Crossfire).\n",
    "        # link_cp_bics = [9, 14, 15, 25, 26, 26, 28, 30, 33, 34, 36, 37, 38, 39, 40,\n",
    "        #                     42, 42, 43, 45, 46, 48, 50, 53, 57, 62, 63, 64, 65, 66, 67, \n",
    "        #                     75, 78, 82, 97, 98, 102, 103, 114, 116, 124, 129, 137, 170, \n",
    "        #                     176, 205, 224, 263, 277]\n",
    "        # link_cp_uscarrier = [2, 2, 4, 19, 24, 31, 44, 64, 91, 96, 108, 123, 130, \n",
    "        #                          148, 166, 177, 180, 184, 216, 218, 219, 241, 254, 255, \n",
    "        #                          266, 277, 281, 283, 304, 306, 310, 312, 312, 312, 312, \n",
    "        #                          313, 314, 314, 314, 314, 314, 314, 314, 314, 314, 317, \n",
    "        #                          317, 317, 318, 325, 329, 342, 343, 355, 358, 387, 392, \n",
    "        #                          405, 410, 413, 418, 443, 444, 449, 476, 493, 493, 501, \n",
    "        #                          505, 508, 513, 520, 531, 533, 536, 575, 576, 592, 592, \n",
    "        #                          615, 615, 617, 620, 624, 624, 624, 624, 624, 640, 641, \n",
    "        #                          666, 685, 689, 704, 706, 714, 733, 736, 738, 750, 788, \n",
    "        #                          790, 804, 866, 890, 908, 915, 930, 930, 930, 930, 930, \n",
    "        #                          930, 945, 984, 997, 1002, 1022, 1050, 1074, 1076, 1096, \n",
    "        #                          1188, 1190, 1226, 1232, 1232, 1232, 1232, 1274, 1330, \n",
    "        #                          1456, 1458, 1480, 1492, 1530, 1530, 1530, 1544, 1610, \n",
    "        #                          1774, 1778, 1820, 1824, 1824, 1869, 1883, 1888, 1951, \n",
    "        #                          2191, 2227, 2442, 2570, 2608, 2642, 2651, 2674, 2815, \n",
    "        #                          2916, 3031, 3209, 3504, 3808, 4004, 4216, 4290, 4544, \n",
    "        #                          4571, 4619, 4675, 4794,4813, 4887, 4975, 5857, 6026, \n",
    "        #                          6113, 6179, 6210, 6257, 6434, 6508, 6738, 6832, 7244, \n",
    "        #                          9173, 9279, 9393, 11404]\n",
    "        #  link_cp_viatel = [82, 168, 262, 389, 423, 441, 461, 463, 477, 503, 523, 559, \n",
    "        #                       571, 613, 635, 679, 705, 711, 733, 749, 755, 758, 781, 787, \n",
    "        #                       791, 795, 802, 815, 852, 854, 855, 858, 873, 879, 893, 896, \n",
    "        #                       897, 910, 918, 933, 950, 977, 982, 986, 997, 999, 1002, 1007, \n",
    "        #                       1016, 1050, 1067, 1083, 1086, 1094, 1115, 1119, 1121, 1124, \n",
    "        #                       1131, 1139, 1145, 1191, 1211, 1215, 1223, 1285, 1299, 1305, \n",
    "        #                       1331, 1359, 1367, 1383, 1407, 1413, 1437, 1444, 1451, 1467, \n",
    "        #                       1517, 1533, 1536, 1543, 1623, 1627, 1705, 1735, \n",
    "        #                       1787, 1843, 2489, 2549, 2607, 2667]\n",
    "        phy_topo = assign_ip_addr_randomly(phy_topo,'ipaddr_13576.csv')\n",
    "        # you can change the ipaddr of the CAIDA dataset.\n",
    "        # The influence should be trivial.\n",
    "        if atf_set == []:\n",
    "            print(\"Something wrong, atf set is missing...\")\n",
    "            atf_set = bot_init_debugs(phy_topo)\n",
    "        # This code will generate bots randomly around the topology.\n",
    "    \n",
    "    # The following 2 steps are the CrossFire attack.\n",
    "    # Give the second attribute of link_map to show which route should be use.\n",
    "    # if \"eq\", link map use route_eq for attack.\n",
    "    # if \"nh\", link map use route_nh for attack.\n",
    "    if route_name == 'eq':\n",
    "        e_list = link_map_crossfire(atf_set)\n",
    "        for e in e_list:\n",
    "            for atf in atf_set:\n",
    "                #print(e,\"e.cp_num=\",e.cp_num)\n",
    "                e_tmp = get_subnet_edge_from_list(atf.route_eq)\n",
    "                e_new = edg(_pre(e.src),_pre(e.dst))\n",
    "                #print(e_list,e_new)\n",
    "                if e_new in e_tmp:\n",
    "                    e.cp_num += 1\n",
    "        e_list.sort(reverse=True,key = lambda x:x.fd+x.cp_num)\n",
    "    elif route_name == 'nh':\n",
    "        e_list = link_map(atf_set,route_name)\n",
    "        e_list.sort(reverse=True)\n",
    "    potential_tar = e_list[0:5]   \n",
    "    if DEBUG:\n",
    "        # In DEBUG mode you can compare the probed edges and the real edges. \n",
    "        e_list_actual = link_map_actual_route(atf_set)\n",
    "    \n",
    "    # In default, CrossPoint attacks try to check 5 links with side-channel.\n",
    "    # (despite there are hundreds of links in large networks) \n",
    "    # Check more links are easy, and do not have any risks being discovered.\n",
    "    for e in e_list:\n",
    "        print(e)\n",
    "    for e in potential_tar:\n",
    "        \n",
    "        # This step get the remained attack flows that does not pass the target egde e.\n",
    "        # The remained attack flows might pass edge e but hided by defenders, we need to figure it out.\n",
    "        candidate_atfs = get_individual_atfs(e,atf_set)\n",
    "        \n",
    "        # You can try one of the statistical dispairities against PTO defenses.\n",
    "        candidate_atfs.sort(reverse=True,key=lambda x:x.get_sd_ip())\n",
    "        # candidate_atfs.sort(reverse=True,key=lambda x:x.get_sd_nh())\n",
    "        attack_atfs = []\n",
    "        ##### Important #####\n",
    "        # Directly attack with SD, does not base on any congestion. \n",
    "        attack_atfs = get_number_of_bots(candidate_atfs, budget - e.fd)\n",
    "        print(\"Attack flows number :\", len(attack_atfs), budget, e.fd)\n",
    "        if DEBUG:\n",
    "            succ_bot = botnum_success(attack_atfs,e,route_name)\n",
    "            print(f\"edge {e} Success bot number =\", succ_bot +e.fd)\n",
    "            if e.fd + succ_bot >= Link_capacity:\n",
    "                print(\"In the example, you find sufficient bots against a link!\")\n",
    "                return\n",
    "            else:\n",
    "                print(\"Attack failed, try again with larger budget.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "7408b2c4",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start attack..\n",
      "Init atf set finished.\n",
      "(139.60.110.217,198.74.30.62),flow_density = 14,atf_set = [(10, 0), (10, 4), (10, 2), (2, 8), (2, 10), (6, 10), (0, 8), (10, 6), (6, 8), (8, 6), (4, 9), (8, 0), (8, 4), (8, 2)]\n",
      "(198.74.30.44,139.60.110.7),flow_density = 11,atf_set = [(2, 9), (4, 10), (8, 5), (10, 5), (8, 3), (10, 3), (0, 10), (10, 1), (8, 1), (6, 9), (4, 8)]\n",
      "(198.74.30.62,216.106.201.6),flow_density = 9,atf_set = [(10, 0), (10, 4), (10, 2), (2, 8), (3, 8), (2, 10), (1, 6), (1, 10), (4, 9)]\n",
      "(198.74.30.44,216.106.201.6),flow_density = 9,atf_set = [(6, 2), (8, 3), (6, 4), (9, 2), (0, 10), (2, 6), (8, 1), (0, 6), (6, 0)]\n",
      "(216.106.201.151,198.74.30.44),flow_density = 8,atf_set = [(2, 9), (4, 10), (3, 10), (10, 3), (10, 1), (1, 8), (3, 6), (4, 8)]\n",
      "(198.74.30.62,216.106.201.151),flow_density = 8,atf_set = [(9, 4), (6, 3), (4, 6), (0, 8), (8, 0), (6, 1), (8, 4), (8, 2)]\n",
      "(139.60.110.217,198.74.30.44),flow_density = 5,atf_set = [(9, 6), (9, 2), (3, 10), (1, 8), (5, 8)]\n",
      "(139.60.110.7,198.74.30.62),flow_density = 4,atf_set = [(9, 4), (3, 8), (1, 10), (5, 10)]\n",
      "(68.168.237.182,216.106.201.151),flow_density = 8,atf_set = [(5, 4), (4, 1), (6, 3), (10, 3), (1, 8), (3, 2), (1, 4), (2, 5)]\n",
      "(68.168.237.214,216.106.201.151),flow_density = 8,atf_set = [(3, 10), (2, 3), (10, 1), (4, 3), (7, 2), (6, 1), (2, 7), (3, 6)]\n",
      "(208.53.239.82,68.168.237.182),flow_density = 7,atf_set = [(7, 1), (7, 3), (3, 9), (0, 9), (2, 5), (0, 5), (3, 5)]\n",
      "(208.53.239.137,68.168.237.182),flow_density = 7,atf_set = [(5, 4), (9, 3), (5, 0), (5, 2), (3, 7), (9, 0), (4, 5)]\n",
      "(68.168.237.182,66.6.15.29),flow_density = 9,atf_set = [(4, 1), (1, 6), (1, 10), (8, 1), (1, 8), (3, 0), (0, 9), (1, 4), (0, 5)]\n",
      "(66.6.15.151,68.168.237.214),flow_density = 9,atf_set = [(1, 5), (1, 9), (1, 3), (3, 1), (10, 1), (1, 7), (0, 7), (6, 1), (0, 3)]\n",
      "(68.168.237.214,208.53.239.137),flow_density = 7,atf_set = [(1, 5), (7, 4), (1, 9), (7, 0), (0, 7), (7, 2), (2, 7)]\n",
      "(216.106.201.6,139.60.109.82),flow_density = 9,atf_set = [(6, 2), (10, 2), (2, 8), (9, 2), (2, 10), (5, 2), (2, 6), (2, 4), (4, 0)]\n",
      "(208.53.239.82,68.168.237.214),flow_density = 5,atf_set = [(5, 3), (9, 1), (4, 7), (1, 7), (5, 1)]\n",
      "(139.60.109.195,216.106.201.151),flow_density = 7,atf_set = [(2, 9), (2, 3), (3, 2), (7, 2), (2, 5), (2, 7), (8, 2)]\n",
      "(216.106.201.6,68.168.237.214),flow_density = 5,atf_set = [(8, 3), (7, 4), (3, 8), (4, 7), (3, 4)]\n",
      "(68.168.237.182,216.106.201.6),flow_density = 5,atf_set = [(1, 6), (5, 2), (1, 10), (8, 1), (4, 5)]\n",
      "(216.106.201.6,139.60.109.195),flow_density = 5,atf_set = [(10, 0), (4, 2), (0, 10), (0, 6), (6, 0)]\n",
      "(75.102.167.147,208.53.239.137),flow_density = 7,atf_set = [(7, 6), (7, 4), (1, 9), (7, 0), (3, 7), (7, 2), (5, 9)]\n",
      "(75.102.167.27,208.53.239.82),flow_density = 6,atf_set = [(7, 1), (7, 3), (7, 5), (6, 7), (1, 7), (3, 9)]\n",
      "(68.168.237.182,66.6.15.151),flow_density = 3,atf_set = [(7, 1), (5, 0), (9, 0)]\n",
      "(208.53.239.137,75.102.167.27),flow_density = 5,atf_set = [(5, 7), (9, 3), (0, 7), (9, 0), (2, 7)]\n",
      "(68.168.237.214,66.6.15.29),flow_density = 3,atf_set = [(9, 1), (7, 0), (5, 1)]\n",
      "(139.60.109.82,216.106.201.151),flow_density = 3,atf_set = [(0, 4), (0, 8), (8, 0)]\n",
      "(75.102.167.147,208.53.239.82),flow_density = 4,atf_set = [(9, 1), (4, 7), (9, 5), (0, 9)]\n",
      "(139.60.110.7,204.147.185.66),flow_density = 9,atf_set = [(4, 10), (10, 5), (8, 10), (10, 8), (10, 3), (0, 10), (10, 1), (1, 10), (5, 10)]\n",
      "(204.147.185.66,139.60.110.217),flow_density = 7,atf_set = [(10, 0), (10, 4), (10, 2), (3, 10), (2, 10), (6, 10), (10, 6)]\n",
      "(75.102.167.147,216.249.200.6),flow_density = 8,atf_set = [(7, 9), (9, 1), (1, 9), (9, 5), (10, 7), (8, 7), (0, 9), (5, 9)]\n",
      "(216.249.200.6,75.102.167.27),flow_density = 6,atf_set = [(9, 3), (3, 9), (9, 0), (7, 8), (7, 10), (9, 7)]\n",
      "(139.60.109.195,66.231.19.114),flow_density = 6,atf_set = [(10, 0), (0, 2), (2, 0), (0, 10), (0, 6), (6, 0)]\n",
      "(66.6.15.151,66.231.19.114),flow_density = 6,atf_set = [(1, 0), (5, 0), (0, 7), (9, 0), (2, 1), (0, 3)]\n",
      "(66.231.19.114,139.60.109.82),flow_density = 6,atf_set = [(0, 4), (0, 8), (1, 2), (4, 0), (8, 0), (2, 1)]\n",
      "(66.6.15.29,66.231.19.114),flow_density = 6,atf_set = [(7, 0), (0, 1), (1, 2), (3, 0), (0, 9), (0, 5)]\n",
      "(139.60.110.7,216.249.200.6),flow_density = 6,atf_set = [(2, 9), (9, 4), (8, 7), (9, 8), (7, 8), (6, 9)]\n",
      "(216.249.200.6,139.60.110.217),flow_density = 4,atf_set = [(9, 6), (9, 2), (8, 9), (4, 9)]\n",
      "(198.74.30.44,208.53.239.82),flow_density = 3,atf_set = [(8, 5), (6, 7), (5, 8)]\n",
      "(208.53.239.137,198.74.30.44),flow_density = 2,atf_set = [(7, 6), (10, 5)]\n",
      "(208.53.239.82,198.74.30.62),flow_density = 2,atf_set = [(5, 6), (5, 10)]\n",
      "(198.74.30.62,208.53.239.137),flow_density = 1,atf_set = [(6, 5)]\n",
      "(216.249.200.6,204.147.185.66),flow_density = 4,atf_set = [(9, 10), (10, 9), (10, 7), (7, 10)]\n",
      "Attack flows number : 0 11 14\n",
      "edge (139.60.110.217,198.74.30.62),flow_density = 14,atf_set = [(10, 0), (10, 4), (10, 2), (2, 8), (2, 10), (6, 10), (0, 8), (10, 6), (6, 8), (8, 6), (4, 9), (8, 0), (8, 4), (8, 2)] Success bot number = 14\n",
      "Attack failed, try again with larger budget.\n",
      "Attack flows number : 0 11 11\n",
      "edge (198.74.30.44,139.60.110.7),flow_density = 11,atf_set = [(2, 9), (4, 10), (8, 5), (10, 5), (8, 3), (10, 3), (0, 10), (10, 1), (8, 1), (6, 9), (4, 8)] Success bot number = 11\n",
      "Attack failed, try again with larger budget.\n",
      "Attack flows number : 2 11 9\n",
      "edge (198.74.30.62,216.106.201.6),flow_density = 9,atf_set = [(10, 0), (10, 4), (10, 2), (2, 8), (3, 8), (2, 10), (1, 6), (1, 10), (4, 9)] Success bot number = 9\n",
      "Attack failed, try again with larger budget.\n",
      "Attack flows number : 2 11 9\n",
      "edge (198.74.30.44,216.106.201.6),flow_density = 9,atf_set = [(6, 2), (8, 3), (6, 4), (9, 2), (0, 10), (2, 6), (8, 1), (0, 6), (6, 0)] Success bot number = 9\n",
      "Attack failed, try again with larger budget.\n",
      "Attack flows number : 3 11 8\n",
      "edge (216.106.201.151,198.74.30.44),flow_density = 8,atf_set = [(2, 9), (4, 10), (3, 10), (10, 3), (10, 1), (1, 8), (3, 6), (4, 8)] Success bot number = 9\n",
      "Attack failed, try again with larger budget.\n"
     ]
    }
   ],
   "source": [
    "# Run attack against equalNet:\n",
    "# Change the second value of budget to trying to 'attack' the network!\n",
    "# This example uses a simple topology named Abilene. \n",
    "# You can run Nethide or EqualNet to create more complex topologies to test our CrossPoint attack!\n",
    "CrossPoint_Attack_SD('./botconfig-example-equalnet/',15,'eq')\n",
    "# Note : basic Link capacity is set to 15, in this example.\n",
    "# So your budget must large then 15 in this case.\n",
    "# Note: CrossPoint-SD attack not very effcient in some times, as our exeriments show: (Figure9-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "10e95db4",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start attack..\n",
      "Init atf set finished.\n",
      "(7,10),flow_density = 10,atf_set = [(7, 1), (4, 10), (8, 10), (3, 10), (10, 8), (6, 10), (10, 7), (1, 8), (5, 10), (7, 10)]\n",
      "(10,1),flow_density = 10,atf_set = [(7, 1), (4, 1), (9, 1), (1, 9), (0, 10), (10, 1), (1, 10), (1, 8), (2, 1), (6, 1)]\n",
      "(5,4),flow_density = 10,atf_set = [(5, 4), (3, 8), (5, 6), (3, 9), (4, 9), (4, 5), (2, 5), (0, 5), (3, 5), (4, 8)]\n",
      "(2,9),flow_density = 10,atf_set = [(2, 9), (4, 2), (9, 1), (2, 8), (9, 2), (2, 10), (0, 8), (3, 2), (0, 9), (8, 2)]\n",
      "(7,8),flow_density = 10,atf_set = [(7, 9), (8, 10), (10, 8), (8, 7), (1, 8), (6, 8), (5, 10), (8, 0), (7, 8), (6, 9)]\n",
      "(8,9),flow_density = 10,atf_set = [(7, 9), (2, 8), (0, 8), (8, 9), (3, 9), (9, 8), (4, 9), (6, 9), (5, 9), (8, 2)]\n",
      "(0,2),flow_density = 10,atf_set = [(0, 2), (2, 0), (6, 2), (10, 2), (0, 8), (5, 2), (1, 2), (7, 2), (6, 0), (0, 9)]\n",
      "(7,6),flow_density = 10,atf_set = [(7, 3), (4, 10), (7, 6), (6, 7), (7, 4), (3, 10), (6, 10), (6, 8), (7, 2), (6, 9)]\n",
      "(8,5),flow_density = 10,atf_set = [(1, 5), (8, 5), (3, 8), (3, 9), (4, 9), (5, 10), (5, 8), (5, 9), (5, 1), (4, 8)]\n",
      "(4,6),flow_density = 10,atf_set = [(4, 10), (10, 4), (7, 4), (6, 4), (0, 4), (5, 6), (4, 6), (10, 6), (2, 4), (1, 4)]\n",
      "(6,3),flow_density = 7,atf_set = [(7, 3), (9, 6), (6, 3), (1, 3), (3, 10), (3, 6), (0, 3)]\n",
      "(9,10),flow_density = 7,atf_set = [(9, 10), (1, 9), (10, 9), (2, 10), (2, 3), (10, 3), (9, 0)]\n",
      "(0,6),flow_density = 7,atf_set = [(10, 4), (6, 2), (0, 4), (2, 4), (0, 6), (7, 2), (0, 3)]\n",
      "(1,0),flow_density = 7,atf_set = [(1, 0), (0, 10), (7, 0), (0, 1), (0, 7), (1, 2), (8, 0)]\n",
      "(9,3),flow_density = 6,atf_set = [(5, 3), (9, 6), (9, 3), (2, 3), (10, 3), (3, 2)]\n",
      "(4,3),flow_density = 6,atf_set = [(8, 3), (3, 8), (3, 4), (4, 3), (3, 9), (3, 5)]\n",
      "(2,10),flow_density = 5,atf_set = [(9, 1), (2, 3), (2, 4), (2, 1), (6, 1)]\n",
      "(1,7),flow_density = 5,atf_set = [(4, 7), (7, 0), (1, 7), (0, 7), (8, 0)]\n",
      "(3,0),flow_density = 4,atf_set = [(10, 0), (5, 0), (9, 0), (3, 0)]\n",
      "(2,7),flow_density = 4,atf_set = [(5, 7), (3, 7), (2, 7), (9, 7)]\n",
      "(5,2),flow_density = 3,atf_set = [(5, 7), (3, 7), (9, 7)]\n",
      "(1,8),flow_density = 3,atf_set = [(1, 5), (8, 1), (5, 1)]\n",
      "(10,0),flow_density = 3,atf_set = [(10, 4), (10, 2), (2, 4)]\n",
      "(1,6),flow_density = 3,atf_set = [(1, 3), (1, 6), (1, 4)]\n",
      "(2,6),flow_density = 3,atf_set = [(2, 6), (6, 0), (6, 1)]\n",
      "(5,9),flow_density = 2,atf_set = [(5, 3), (9, 7)]\n",
      "(10,3),flow_density = 2,atf_set = [(10, 0), (9, 0)]\n",
      "(4,10),flow_density = 2,atf_set = [(4, 1), (10, 6)]\n",
      "(9,4),flow_density = 2,atf_set = [(9, 4), (4, 2)]\n",
      "(8,4),flow_density = 2,atf_set = [(8, 3), (8, 4)]\n",
      "(5,3),flow_density = 2,atf_set = [(5, 0), (3, 7)]\n",
      "(6,5),flow_density = 2,atf_set = [(9, 5), (6, 5)]\n",
      "(4,0),flow_density = 2,atf_set = [(4, 0), (0, 5)]\n",
      "(7,5),flow_density = 1,atf_set = [(7, 5)]\n",
      "(10,5),flow_density = 1,atf_set = [(10, 5)]\n",
      "(3,1),flow_density = 1,atf_set = [(3, 1)]\n",
      "(4,1),flow_density = 1,atf_set = [(4, 7)]\n",
      "(9,6),flow_density = 1,atf_set = [(9, 5)]\n",
      "(5,0),flow_density = 1,atf_set = [(5, 2)]\n",
      "(8,6),flow_density = 1,atf_set = [(8, 6)]\n",
      "(2,4),flow_density = 1,atf_set = [(2, 5)]\n",
      "Attack flows number : 20 30 10\n",
      "edge (7,10),flow_density = 10,atf_set = [(7, 1), (4, 10), (8, 10), (3, 10), (10, 8), (6, 10), (10, 7), (1, 8), (5, 10), (7, 10)] Success bot number = 14\n",
      "Attack failed, try again with larger budget.\n",
      "Attack flows number : 20 30 10\n",
      "edge (10,1),flow_density = 10,atf_set = [(7, 1), (4, 1), (9, 1), (1, 9), (0, 10), (10, 1), (1, 10), (1, 8), (2, 1), (6, 1)] Success bot number = 12\n",
      "Attack failed, try again with larger budget.\n",
      "Attack flows number : 20 30 10\n",
      "edge (5,4),flow_density = 10,atf_set = [(5, 4), (3, 8), (5, 6), (3, 9), (4, 9), (4, 5), (2, 5), (0, 5), (3, 5), (4, 8)] Success bot number = 12\n",
      "Attack failed, try again with larger budget.\n",
      "Attack flows number : 20 30 10\n",
      "edge (2,9),flow_density = 10,atf_set = [(2, 9), (4, 2), (9, 1), (2, 8), (9, 2), (2, 10), (0, 8), (3, 2), (0, 9), (8, 2)] Success bot number = 10\n",
      "Attack failed, try again with larger budget.\n",
      "Attack flows number : 20 30 10\n",
      "edge (7,8),flow_density = 10,atf_set = [(7, 9), (8, 10), (10, 8), (8, 7), (1, 8), (6, 8), (5, 10), (8, 0), (7, 8), (6, 9)] Success bot number = 15\n",
      "In the example, you find sufficient bots against a link!\n"
     ]
    }
   ],
   "source": [
    "# Run attack against Nethide:\n",
    "CrossPoint_Attack_SD('./botconfig-example-nethide/',30,'nh')\n",
    "# Note : basic Link capacity is set to 15, in this example.\n",
    "# So your budget must large then 15 in this case.\n",
    "# Note: CrossPoint-SD attack not very effcient in some times, as our exeriments show: (Figure9-10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
