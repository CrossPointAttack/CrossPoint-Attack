{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98e1329a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code is open source to USENIX Security submission only (now).  \n",
    "# After the publication of the corresponding thesis, the code will be free to use, modify, and distribute.\n",
    "# Copyright (c) [Anonymous authors] [2023], which will be public after the publication.\n",
    "# Licensed under the MIT License, Version 2.0.\n",
    "# We kindly hope that this code should only be used for academic perspectives, \n",
    "# Please do not abuse this code to launch any attacks. \n",
    "\n",
    "# This code is the CrossPoint controller CC attacks!\n",
    "# After you collect the bot-config files, you can run this file to reveal hidden links.\n",
    "# Input: 1. bot-config files (JSON) should be in the correct place.\n",
    "#        2. Optionally, you can have CSV files that represent the result of congestion.\n",
    "#        3. Your budget.\n",
    "# Output: 1. The control group.\n",
    "#         2. The attack bots (using CC,SD or both.)\n",
    "\n",
    "# CrossPoint attacks workflow: \n",
    "\n",
    "# For researchers who want to rebuild our experiments in your own environment:\n",
    "    # 1. Run the bot_config in each bot, generating attack_flow JSON files. \n",
    "    # 2. Send these JSON files to the controller. \n",
    "    # 3. In the controller, run controller_CrossPoint to output the suspicious attack flow set. (This file)\n",
    "    # 4. In the controller, find the control group and the attack flow set.\n",
    "    # 5. Run bash_ping in control group bots and suspicious attack flow bots. \n",
    "    # 6. run conrtoller_CrossPoint to find profitable links.\n",
    "\n",
    "# (Fast example) We also give a fast example with configured bots and congestion files!!\n",
    "    # This code gives a fast example to understand CrossPoint-CC attacks!\n",
    "    # This example only contains four bot-paths congestion samples.\n",
    "    # It might take a few minutes to run (the main part is analyzing the congestion). \n",
    "\n",
    "# The experiment can be done with simulations and experiments. \n",
    "# The only difference is in step 2: The method of how you transfer JSON bot config files. \n",
    "\n",
    "# Using this you can run the bash_ping command on a bot to get the csv ping files.\n",
    "# You should add your destination address in the @ip_list variable.\n",
    "\n",
    "import re\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "def raw_ping_to_csv(rawping_file,csv_file_name):\n",
    "    # This function will read a rowping_file and output it to csv ping files.\n",
    "    # Read the file of raw ping data. \n",
    "    # If you do not use our ping tool bash_ping, you will get a raw ping data\n",
    "    # Then you can use this function to transfer the raw ping data to csv files.\n",
    "    # '/home/hxb/CAIDA-dataset/myping_/ping_log_sh_5.18'\n",
    "    with open(rawping_file, 'r') as f:\n",
    "        data = f.readlines()\n",
    "    # Extract the desired features\n",
    "    timestamps = []\n",
    "    icmp_seqs = []\n",
    "    times = []\n",
    "    for row in data:\n",
    "        if 'bytes from' in row:\n",
    "            timestamp = re.search(r\"\\[(.*?)\\]\", row).group(1)\n",
    "            icmp_seq = re.search(r\"icmp_seq=(\\d+)\", row).group(1)\n",
    "            time = re.search(r\"time=(.*?) ms\", row).group(1)\n",
    "            timestamps.append(timestamp)\n",
    "            icmp_seqs.append(icmp_seq)\n",
    "            times.append(time)\n",
    "    # Create a DataFrame\n",
    "    df = pd.DataFrame({'time': timestamps, 'seq': icmp_seqs, 'delay': times})\n",
    "    # Save to a new CSV file\n",
    "    df.to_csv(csv_file_name+'.csv', index=False)\n",
    "    return \n",
    "\n",
    "class congestion:\n",
    "    def __init__(self,start_time,congestion_sample,length,coef_start_time,minrtt) -> None:\n",
    "        self.length = length\n",
    "        # Most importantly, the start_time must correct. \n",
    "        # Most importantly, the start_time must correct. \n",
    "        # It is the one of the main feature that influence the accuracy of CrossPoint attacks. \n",
    "        # (One of the authors wrote a bug here ... We spent almost one week to figure it out....)\n",
    "        self.start_time = start_time\n",
    "        self.congestion_sample = congestion_sample\n",
    "        self.coef_sample = [minrtt]*10 + congestion_sample + [minrtt]*10\n",
    "        self.coef_start_time = coef_start_time\n",
    "        \n",
    "    def update_bucket(self,value):\n",
    "        self.congestion_sample = [value if x == -1 else x for x in self.congestion_sample]\n",
    "            \n",
    "    def update_length(self):\n",
    "        # Not use in the latest version for CCS submission\n",
    "        assert(len(self.delta_rtt) == len(self.drop))\n",
    "        self.length = len(self.delta_rtt)\n",
    "        mow = int(self.length / 10 + 1) \n",
    "        \n",
    "    def get_coef_trace(self):\n",
    "        # Not use in the latest version for CCS submission\n",
    "        return self.coef_rtt_before + self.delta_rtt + self.coef_rtt_after \n",
    "\n",
    "    def update_coef_list(self,tr_list):\n",
    "        # Not use in the latest version for CCS submission\n",
    "        assert(self.length!=0)\n",
    "        coef_before_t = max(self.start_time - self.length * 0.1, 0)\n",
    "        coef_after_t = min(self.start_time + self.length * 0.2, len(tr_list.tr_list))\n",
    "        idx_before = tr_list.get_trace_from_time(coef_before_t)\n",
    "        idx_after = tr_list.get_trace_from_time(coef_after_t)\n",
    "        c = 0\n",
    "        while c < self.length:\n",
    "            self.coef_rtt_before.append(tr_list.tr_list[idx_before].rtt)\n",
    "            self.coef_rtt_after.append(tr_list.tr_list[idx_after].rtt)\n",
    "            c +=1\n",
    "            idx_before +=1\n",
    "            idx_after +=1\n",
    "        return 0\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        s = f'Congestion {self.start_time} inteval {self.length} rtt {self.congestion_sample} '\n",
    "        return s\n",
    "\n",
    "class correlated_congestion:\n",
    "    \n",
    "    def __init__(self,start_time,member_congestion):\n",
    "        self.start_time = start_time\n",
    "        self.member_congestion = member_congestion\n",
    "\n",
    "    @classmethod\n",
    "    def from_json(cls, json_str):\n",
    "        json_dict = json.loads(json_str)\n",
    "        return cls(**json_dict)\n",
    "\n",
    "    def to_json(self):\n",
    "        return json.dumps(self.__dict__)\n",
    "\n",
    "\n",
    "def get_minRTT(trace):\n",
    "    # The minimal rtt of a trace represents the propagation delay.\n",
    "    delays = trace[\"delay\"]\n",
    "    return delays.min()\n",
    "\n",
    "def get_maxRTT(trace):\n",
    "    # The maximal rtt of a trace represents the egde value of RTT near a packet loss.\n",
    "    # Therefore, we use the maximal value of rtt to change the lost dropped packet's rtt.\n",
    "    delays = trace[\"delay\"]\n",
    "    return delays.max()\n",
    "\n",
    "def get_local_max(trace,idx):\n",
    "    # Local max delay parameter indicates the edge value of a packet loss. \n",
    "    delays = trace[\"delay\"].iloc[idx-5:idx+5]\n",
    "    return delays.max()\n",
    "\n",
    "def get_loss(trace):\n",
    "    seq = trace[\"seq\"]\n",
    "    c = 0\n",
    "    for idx,s in enumerate(seq):\n",
    "        if idx == 0:\n",
    "            continue\n",
    "        loss_idt = seq[idx] - seq[idx-1]\n",
    "        #print(loss_idt)\n",
    "        if loss_idt > 1 :\n",
    "            c += 1\n",
    "    return c\n",
    "            \n",
    "def get_abnRTT(trace):\n",
    "    d = trace[\"delay\"]\n",
    "    minrtt = get_minRTT(trace)\n",
    "    c = 0\n",
    "    for idx,s in enumerate(d):\n",
    "        if idx == 0:\n",
    "            continue\n",
    "        if s > minrtt + 30:\n",
    "            c += 1\n",
    "    return c    \n",
    "\n",
    "def delay_update(trace,minrtt):\n",
    "    trace.loc[abs(trace[\"delay\"] - minrtt)< 10,'delay'] = minrtt\n",
    "    trace.loc[trace[\"delay\"] < minrtt,'delay'] = minrtt\n",
    "\n",
    "def time_synchronize(trace,propagation_delay_to_target):\n",
    "    # This is a synchonization step for updating the time stamp in the trace.\n",
    "    # This step is optional, it need you know the propagation delay to the target.\n",
    "    # So you need to send extra ping messages and get the minimal delay.\n",
    "    # We reconmand you do that because in evaluations it shows a ~20% performance increases.\n",
    "    # Note that the propagation delay and the min rtt are ms. therefore we * 1000\n",
    "    def algin(row):\n",
    "        return row - minrtt/2 * 0.001 - propagation_delay_to_target * 0.001\n",
    "    \n",
    "    trace[\"time\"] = trace[\"time\"].apply(algin)\n",
    "\n",
    "def init_trace(trace):\n",
    "    def create_time_stamp(start,length):\n",
    "        return [start+0.1 + i*0.1 for i in range(length)]\n",
    "\n",
    "    rtt = trace[\"delay\"]\n",
    "    minrtt = get_minRTT(trace)\n",
    "    idx_loss = trace.index[trace[\"seq\"].diff() > 1]\n",
    "    #print(idx_loss)\n",
    "    while not idx_loss.empty:\n",
    "        idx = idx_loss[0]\n",
    "        #print(idx,seq[idx],seq[idx-1])\n",
    "        diff = trace['seq'][idx] - trace['seq'][idx -1] - 1\n",
    "        insert_df = pd.DataFrame(\n",
    "                                 {\"seq\":range(trace.loc[idx-1,'seq'] + 1, trace.loc[idx,'seq']), \n",
    "                                  'delay':[-1.0] * diff, \n",
    "                                 \"time\":create_time_stamp(trace.loc[idx-1,'time'],diff),\n",
    "                                 \"drop\":1})\n",
    "        trace = pd.concat([trace.iloc[:idx],insert_df,trace.iloc[idx:]]).reset_index(drop=True)\n",
    "        #print(trace.iloc[270:280])\n",
    "        idx_loss = trace.index[trace[\"seq\"].diff() > 1]\n",
    "        #print(idx_loss)\n",
    "    indices = trace.loc[trace['delay'] == -1 ].index\n",
    "    for i in indices:\n",
    "        local_max = get_local_max(trace,i)\n",
    "        trace.loc[i,'delay'] = local_max\n",
    "    #print(trace.iloc[270:280])\n",
    "    return trace\n",
    "\n",
    "def init_congestion(trace_file_name):\n",
    "    congestion_list = []\n",
    "    trace_raw = pd.read_csv(trace_file_name)\n",
    "    trace = init_trace(trace_raw)\n",
    "    #print(trace.iloc[270:300])\n",
    "    seq = trace[\"seq\"]\n",
    "    rtt = trace[\"delay\"]\n",
    "    timestamp = trace[\"time\"]\n",
    "    minrtt = get_minRTT(trace)\n",
    "    #maxrtt = get_maxRTT(trace)\n",
    "    \n",
    "    #minrtt = 56 # debug\n",
    "    print(f\"minrtt is {minrtt}\")\n",
    "    delay_update(trace,minrtt)\n",
    "    #print(minrtt)\n",
    "\n",
    "    # We set the congestion as delay increase more than 30\\% of the minimal RTT.\n",
    "    # In most congestion control thesis, congestion is any condition when delay > minimal RTT.\n",
    "    delay_idt_thre = min([0.3 * minrtt,30])\n",
    "    #print(delay_idt_thre)\n",
    "    idx = 1 \n",
    "    while idx < len(seq):\n",
    "        # loss indicator to judge whether congestion happens.\n",
    "        loss_idt = seq[idx] - seq[idx - 1]\n",
    "        # delay indicator to judge whether congestion happens.\n",
    "        delay_idt = rtt[idx] - minrtt\n",
    "        # If lost package or delay is high\n",
    "        if loss_idt > 1 or delay_idt > delay_idt_thre:\n",
    "            c_flag = True\n",
    "            congestion_start_time = timestamp[idx]\n",
    "            congestion_sample = []\n",
    "            if loss_idt > 1:\n",
    "                #maxrtt = get_local_max(trace,idx)\n",
    "                maxrtt = max(rtt[idx],rtt[idx-1])\n",
    "                #maxrtt = rtt[idx]\n",
    "                sample = [maxrtt] * (loss_idt - 1)\n",
    "                congestion_sample += sample\n",
    "            if delay_idt > delay_idt_thre:\n",
    "                #congestion_sample.append(delay_idt // delay_idt_thre)\n",
    "                congestion_sample.append(rtt[idx])\n",
    "            leng_idx = idx\n",
    "            # A congestion starts, judge whether the following trace belongs to this congestion.\n",
    "            while c_flag:\n",
    "                idx += 1\n",
    "                if idx >= len(seq):\n",
    "                    break\n",
    "                loss_idt = seq[idx] - seq[idx - 1]\n",
    "                delay_idt = rtt[idx] - minrtt\n",
    "                if loss_idt > 1 or delay_idt > delay_idt_thre:\n",
    "                    c_flag = True\n",
    "                    if loss_idt > 1:\n",
    "                        maxrtt = max(rtt[idx],rtt[idx-1])\n",
    "                        sample = [maxrtt] * (loss_idt - 1)\n",
    "                        congestion_sample += sample\n",
    "                    if delay_idt > delay_idt_thre:\n",
    "                        #congestion_sample.append(delay_idt // delay_idt_thre)\n",
    "                        congestion_sample.append(rtt[idx])\n",
    "                    continue\n",
    "                else:\n",
    "                    c_flag = False\n",
    "                    congestion_length = len(congestion_sample)\n",
    "                    coef_start_time = congestion_start_time - 1.0\n",
    "                    cgt = congestion(congestion_start_time, \\\n",
    "                                     congestion_sample,congestion_length, \\\n",
    "                                    coef_start_time, minrtt)\n",
    "                    congestion_list.append(cgt)\n",
    "        idx += 1\n",
    "    return congestion_list\n",
    "\n",
    "def search_time_trace(trace,start_time):\n",
    "    # Given a start_time (from another bot), we need to search the rtt samples in this bot.\n",
    "    # And then we can use them to judge the coefficiency. \n",
    "    new_df = trace[abs(trace[\"time\"] - start_time) < 0.1]\n",
    "    if not new_df.empty:\n",
    "        idx = (new_df[\"time\"] - start_time).abs().idxmin()\n",
    "        #result = new_df.loc[idx]\n",
    "        #print(\"idx\",idx,\"time {:.3f}\".format(new_df[\"time\"][idx]))\n",
    "        return idx\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def prepare_coef_sample(trace,trace_start_time,length):\n",
    "    # Trace : the whole csv trace\n",
    "    # teace_start_time: the congestion.coef_start_time \n",
    "    # length: the congestion.length\n",
    "    idx_base = search_time_trace(trace,trace_start_time)\n",
    "    if idx_base == None:\n",
    "        return []\n",
    "    if idx_base - 10 < 0 or idx_base + length + 10 > len(trace):\n",
    "        return []\n",
    "    seq = trace[\"seq\"]\n",
    "    rtt = trace[\"delay\"]\n",
    "    timestamp = trace[\"time\"]\n",
    "    drop = trace[\"drop\"]\n",
    "    minrtt = get_minRTT(trace)\n",
    "    delay_update(trace,minrtt)\n",
    "    #idx = trace.loc[trace['time'] == coef_sample_start].index[0]\n",
    "    delay_idt_thre = min([0.3 * minrtt,30])\n",
    "    coef_sample = []\n",
    "    idx = idx_base\n",
    "    drop_count = 0\n",
    "    while len(coef_sample) < length:\n",
    "        coef_sample.append(rtt[idx])\n",
    "        idx += 1\n",
    "        drop_count += drop[idx]\n",
    "    if drop_count > len(coef_sample)*0.8:\n",
    "        return []\n",
    "    nan_flag = True\n",
    "    for t in coef_sample:\n",
    "        if t != minrtt:\n",
    "            nan_flag = False\n",
    "    if nan_flag:\n",
    "        coef_sample[0] -= random.random() / 10\n",
    "    return coef_sample\n",
    "\n",
    "def c_group_congestions(C_group_congestion_file,target_name) -> list:\n",
    "    # Given members in C-group, this function generates a congestion samples list,\n",
    "    # And save it to a JSON file. \n",
    "    # In CrossPoint attacks, this file should be sent to unknown bots for judging.\n",
    "    # This function might be memory starving, depending on how many members in the c-group.\n",
    "    basic_congestion_list = init_congestion(C_group_congestion_file[0])\n",
    "    idx = 1\n",
    "    tr_list = []\n",
    "    c_group_congestion_list = []\n",
    "    # Init trace_list_files\n",
    "    while idx < len(C_group_congestion_file):\n",
    "        print(\"Start inititial traces, \",idx)\n",
    "        tr_raw = pd.read_csv(C_group_congestion_file[idx])\n",
    "        # The init_trace may need a long time to run.\n",
    "        tr = init_trace(tr_raw)\n",
    "        tr_list.append(tr)\n",
    "        idx += 1\n",
    "    print(\"Init congestion and trace finished\")\n",
    "    for c in basic_congestion_list:\n",
    "        bad_congestion_flag = False  \n",
    "        if not c.length > 1:\n",
    "            # Too short congestion, drop\n",
    "            continue\n",
    "        if c.coef_sample == []:\n",
    "            # Bad congestion, might be the \"host unreadable\" in data.\n",
    "            continue\n",
    "        samples = [c.coef_sample]\n",
    "        for tr in tr_list:\n",
    "            bad_congestion_flag = False\n",
    "            c_sample_tmp = prepare_coef_sample(tr, c.start_time -1.0, c.length + 20)\n",
    "            if c_sample_tmp == []:\n",
    "                bad_congestion_flag = True\n",
    "                # If A observes congestion at 9:00 p.m.\n",
    "                # But B's csv file lasts from 8:00 p.m. to 8:30 p.m. (end before the congestion)\n",
    "                # The prepare_coef_sample will return [] to tell the abnormal condition. \n",
    "                break    \n",
    "            samples.append(c_sample_tmp)\n",
    "        if bad_congestion_flag:\n",
    "            continue\n",
    "        co = np.corrcoef(samples)\n",
    "        dis = 1 - co\n",
    "        max_distance = np.max(dis[np.where(~np.eye(dis.shape[0], dtype=bool))])\n",
    "        if max_distance < 0.5 and max_distance >= 0:\n",
    "            cc = correlated_congestion(c.start_time,samples)\n",
    "            c_group_congestion_list.append(cc)\n",
    "        else:\n",
    "            continue\n",
    "    file_name = '_'.join([target_name,'cc'])        \n",
    "    with open(file_name+'.json','w') as f:\n",
    "        for idx,cc in enumerate(c_group_congestion_list):\n",
    "            file_content = cc.to_json()\n",
    "            json.dump(file_content,f)\n",
    "            f.write('\\n')\n",
    "        f.close()\n",
    "            \n",
    "    return c_group_congestion_list\n",
    "\n",
    "def get_cgroup_congestion(filename):\n",
    "    cc_congestion = []\n",
    "    with open(filename) as infile:\n",
    "        for line in infile:\n",
    "            item = json.loads(line)\n",
    "            cc_instance = correlated_congestion.from_json(item)\n",
    "            cc_congestion.append(cc_instance)\n",
    "        infile.close()\n",
    "    return cc_congestion\n",
    "\n",
    "def bot_judge_congestion(bot_congestion_file,cc_congestion_sample):\n",
    "    trace_raw = pd.read_csv(bot_congestion_file)\n",
    "    trace = init_trace(trace_raw)\n",
    "    Flag_of_true_result = 0\n",
    "    print(\"Trace initialize finished. \")\n",
    "    for cc in cc_congestion_sample:\n",
    "        bad_congestion_flag = False\n",
    "        start_time = cc.start_time\n",
    "        length = len(cc.member_congestion[0])\n",
    "        bot_sample = prepare_coef_sample(trace, start_time -1.0, length)\n",
    "        if bot_sample == []:\n",
    "            bad_congestion_flag = True\n",
    "            continue\n",
    "        samples_all = []\n",
    "        for cm in cc.member_congestion:\n",
    "            samples_all.append(cm)\n",
    "        samples_all.append(bot_sample)\n",
    "        co = np.corrcoef(samples_all)\n",
    "        dis = 1 - co\n",
    "        max_distance = np.max(dis[np.where(~np.eye(dis.shape[0], dtype=bool))])\n",
    "        if max_distance < 0.5 and max_distance >= 0:\n",
    "                Flag_of_true_result += 1\n",
    "    print(f\"Find {Flag_of_true_result} evidence of link sharing.\")\n",
    "    # IF the FLAG >= 1, in CrossPoint CC attack we determine the bot-path hide the target.\n",
    "    return Flag_of_true_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01136ad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trace initialize finished. \n",
      "Find 721 evidence of link sharing.\n",
      "Trace initialize finished. \n",
      "Find 742 evidence of link sharing.\n"
     ]
    }
   ],
   "source": [
    "cc_samples = get_cgroup_congestion(\"congestion-example-csv/cgruop-congestion_cc.json\")\n",
    "# In this example, we use the raw data of our experiments,\n",
    "# In an real attack, t1 and t2 should be ran on individual bots, therefore more fast.\n",
    "# t1 and t2 may cost sereral minutes to run ...\n",
    "t1 = bot_judge_congestion(\"congestion-example-csv/pinglog_botpath1.csv\",cc_samples)\n",
    "t2 = bot_judge_congestion(\"congestion-example-csv/pinglog_botpath2.csv\",cc_samples)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
